{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "\\newcommand{\\bx}{\\mathbf{x}}\n",
    "\\newcommand{\\bv}{\\mathbf{v}}\n",
    "\\newcommand{\\by}{\\mathbf{y}}\n",
    "\\newcommand{\\bz}{\\mathbf{z}}\n",
    "\\newcommand{\\E}{\\mathbb{E}}\n",
    "\\newcommand{\\V}{\\mathbb{V}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\calN}{\\mathcal{N}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple implementation of a Helmholtz machine trained using the wake-sleep algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(self):\n",
    "        self._obs = np.array([\n",
    "            np.array([[1, 0, 0],\n",
    "                      [1, 0, 0],\n",
    "                      [1, 0, 0],]).ravel(),\n",
    "            np.array([[0, 1, 0],\n",
    "                      [0, 1, 0],\n",
    "                      [0, 1, 0],]).ravel(),\n",
    "            np.array([[0, 0, 1],\n",
    "                      [0, 0, 1],\n",
    "                      [0, 0, 1],]).ravel(),\n",
    "            np.array([[1, 1, 0],\n",
    "                      [1, 1, 0],\n",
    "                      [1, 1, 0],]).ravel(),\n",
    "            np.array([[1, 0, 1],\n",
    "                      [1, 0, 1],\n",
    "                      [1, 0, 1],]).ravel(),\n",
    "            np.array([[0, 1, 1],\n",
    "                      [0, 1, 1],\n",
    "                      [0, 1, 1],]).ravel(),\n",
    "            np.array([[1, 1, 1],\n",
    "                      [0, 0, 0],\n",
    "                      [0, 0, 0],]).ravel(),\n",
    "            np.array([[0, 0, 0],\n",
    "                      [1, 1, 1],\n",
    "                      [0, 0, 0],]).ravel(),\n",
    "            np.array([[0, 0, 0],\n",
    "                      [0, 0, 0],\n",
    "                      [1, 1, 1],]).ravel(),\n",
    "            np.array([[1, 1, 1],\n",
    "                      [1, 1, 1],\n",
    "                      [0, 0, 0],]).ravel(),\n",
    "            np.array([[1, 1, 1],\n",
    "                      [0, 0, 0],\n",
    "                      [1, 1, 1],]).ravel(),\n",
    "            np.array([[0, 0, 0],\n",
    "                      [1, 1, 1],\n",
    "                      [1, 1, 1],]).ravel(),\n",
    "        ])\n",
    "        self._probs = np.array([2/18, 2/18, 2/18, 2/18, 2/18, 2/18, 1/18, 1/18, 1/18, 1/18, 1/18, 1/18])\n",
    "        \n",
    "    def sample(self):\n",
    "        idx = np.random.choice(self._obs.shape[0], p=self._probs)\n",
    "        return np.expand_dims(self._obs[idx], axis=0)\n",
    "    \n",
    "    def valid_observations(self):\n",
    "        return self._obs\n",
    "    \n",
    "    def kl(self, other_probs):\n",
    "        return np.sum(self._probs * np.log(self._probs)) - np.sum(self._probs * np.log(other_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidLayer:\n",
    "    def __init__(self, input_size, output_size, include_bias=False):\n",
    "        self._i = input_size\n",
    "        self._o = output_size\n",
    "        self._include_bias = include_bias\n",
    "        \n",
    "        self._output = None\n",
    "        self._initialise_params()\n",
    "    \n",
    "    def _initialise_params(self):\n",
    "        self._W = np.zeros((self._o, self._i))\n",
    "        if self._include_bias:\n",
    "            self._b = np.zeros((self._o, 1))\n",
    "        \n",
    "    def _compute_activities(self, x):\n",
    "        assert len(x.shape) == 2, \"received input of shape {}\".format(x.shape)\n",
    "        assert x.shape[1] == self._i, \"received input of shape {}, expected 2nd dim {}\".format(x.shape, self._i)\n",
    "        \n",
    "        activity = x @ self._W.T\n",
    "        if self._include_bias:\n",
    "            activity += self._b.T\n",
    "        return activity\n",
    "    \n",
    "    def _probs(self, x):\n",
    "        return sigmoid(self._compute_activities(x))\n",
    "    \n",
    "    def _sample(self, x):\n",
    "        probs = self._probs(x)\n",
    "        return np.random.uniform(size=probs.shape) < probs\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self._input = x\n",
    "        self._output = self._sample(x)\n",
    "        return self._output\n",
    "    \n",
    "    def state(self):\n",
    "        assert self._input is not None\n",
    "        assert self._output is not None\n",
    "\n",
    "        return (self._input, self._output)\n",
    "    \n",
    "    def learn(self, true_input, target_output, learning_rate):\n",
    "        prediction_error = target_output - self._probs(true_input)\n",
    "        self._W += learning_rate * prediction_error.T @ true_input\n",
    "        if self._include_bias:\n",
    "            self._b += learning_rate * prediction_error.T @ np.ones((true_input.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SBN:\n",
    "    def __init__(self, layer_sizes, include_bias_layer=False):\n",
    "        assert len(layer_sizes) >= 2\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.include_bias_layer = include_bias_layer\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self):\n",
    "        self.layers = []\n",
    "        if self.include_bias_layer:\n",
    "            self.layers.append(SigmoidLayer(1, self.layer_sizes[0], True))\n",
    "        for input_size, output_size in zip(self.layer_sizes[:-1], self.layer_sizes[1:]):\n",
    "            self.layers.append(SigmoidLayer(input_size, output_size, True))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if self.include_bias_layer and x is None:\n",
    "            x = np.zeros((1, 1))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def state(self):\n",
    "        ret = [self.layers[0].state()[0]]\n",
    "        for layer in self.layers:\n",
    "            ret.append(layer.state()[1])\n",
    "        return ret\n",
    "    \n",
    "    def learn(self, true_inputs, targets, learning_rate):\n",
    "        if self.include_bias_layer:\n",
    "            true_inputs = [np.zeros((true_inputs[0].shape[0], 1))] + true_inputs\n",
    "        assert len(self.layers) == len(true_inputs) == len(targets)\n",
    "        \n",
    "        for layer, true_input, target in zip(self.layers, true_inputs, targets):\n",
    "            layer.learn(true_input, target, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HM:\n",
    "    def __init__(self, layer_sizes):\n",
    "        assert len(layer_sizes) >= 2\n",
    "        self._layer_sizes = layer_sizes\n",
    "        self._generator_network = SBN(layer_sizes, True)\n",
    "        self._recognition_network = SBN(layer_sizes[::-1])\n",
    "        \n",
    "    def _wake(self, observation, learning_rate):\n",
    "        self._recognition_network(observation)\n",
    "        targets = self._recognition_network.state()[::-1]\n",
    "        self._generator_network.learn(targets[:-1], targets, learning_rate)\n",
    "        \n",
    "    def _sleep(self, learning_rate):\n",
    "        self._generator_network(None)\n",
    "        targets = self._generator_network.state()[1:][::-1]\n",
    "        self._recognition_network.learn(targets[:-1], targets[1:], learning_rate)\n",
    "            \n",
    "    def _wake_sleep(self, observation, learning_rate):\n",
    "        self._wake(observation, learning_rate)\n",
    "        self._sleep(learning_rate)\n",
    "        \n",
    "    def learn(self, world, epochs, learning_rate):\n",
    "        for _ in range(epochs):\n",
    "            self._wake_sleep(world.sample(), learning_rate)\n",
    "            \n",
    "    def sample(self, num_samples=1):\n",
    "        return self._generator_network(np.zeros((num_samples, 1)))\n",
    "    \n",
    "    def evaluate(self, world, num_samples=10000):\n",
    "        \"\"\"Computes KL divergence between world and model distributions.\"\"\"\n",
    "        assert num_samples > 0\n",
    "        \n",
    "        valid_obs = world.valid_observations()\n",
    "        \n",
    "        samples = self.sample(num_samples)\n",
    "        \n",
    "        hadamard_distances = valid_obs[:, None] - samples\n",
    "        probs = np.mean(np.logical_not(np.any(hadamard_distances, axis=2)), axis=1)\n",
    "        return world.kl(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "world = World()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hm = HM([1, 6, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence: 2.1264738787155206\n",
      "KL divergence: 1.055114500264255\n",
      "KL divergence: 0.7317817316180122\n",
      "KL divergence: 0.5416706172079699\n",
      "KL divergence: 0.4208250744547746\n",
      "KL divergence: 0.35035638489690957\n",
      "KL divergence: 0.3060245302060456\n",
      "KL divergence: 0.266122195328812\n",
      "KL divergence: 0.24376943987078192\n",
      "KL divergence: 0.21922929582550488\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    hm.learn(world, 5000, 0.05)\n",
    "    kl = hm.evaluate(world, 10000)\n",
    "    print(\"KL divergence: {}\".format(kl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
